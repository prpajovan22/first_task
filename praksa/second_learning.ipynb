{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd;\n",
    "from sklearn.svm import SVC\n",
    "import nltk\n",
    "from nltk.stem import \tWordNetLemmatizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pickle\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>filename</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>new content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>business</td>\n",
       "      <td>001.txt</td>\n",
       "      <td>Ad sales boost Time Warner profit</td>\n",
       "      <td>Quarterly profits at US media giant TimeWarne...</td>\n",
       "      <td>quarterly profit u medium giant timewarner jum...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>002.txt</td>\n",
       "      <td>Dollar gains on Greenspan speech</td>\n",
       "      <td>The dollar has hit its highest level against ...</td>\n",
       "      <td>dollar hit highest level euro almost three mon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>business</td>\n",
       "      <td>003.txt</td>\n",
       "      <td>Yukos unit buyer faces loan claim</td>\n",
       "      <td>The owners of embattled Russian oil giant Yuk...</td>\n",
       "      <td>owner embattled russian oil giant yukos ask bu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>business</td>\n",
       "      <td>004.txt</td>\n",
       "      <td>High fuel prices hit BA's profits</td>\n",
       "      <td>British Airways has blamed high fuel prices f...</td>\n",
       "      <td>british airway blamed high fuel price 40% drop...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>business</td>\n",
       "      <td>005.txt</td>\n",
       "      <td>Pernod takeover talk lifts Domecq</td>\n",
       "      <td>Shares in UK drinks and food firm Allied Dome...</td>\n",
       "      <td>share uk drink food firm allied domecq risen s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2220</th>\n",
       "      <td>tech</td>\n",
       "      <td>397.txt</td>\n",
       "      <td>BT program to beat dialler scams</td>\n",
       "      <td>BT is introducing two initiatives to help bea...</td>\n",
       "      <td>bt introducing two initiative help beat rogue ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2221</th>\n",
       "      <td>tech</td>\n",
       "      <td>398.txt</td>\n",
       "      <td>Spam e-mails tempt net shoppers</td>\n",
       "      <td>Computer users across the world continue to i...</td>\n",
       "      <td>computer user across world continue ignore sec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2222</th>\n",
       "      <td>tech</td>\n",
       "      <td>399.txt</td>\n",
       "      <td>Be careful how you code</td>\n",
       "      <td>A new European directive could put software w...</td>\n",
       "      <td>new european directive could put software writ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2223</th>\n",
       "      <td>tech</td>\n",
       "      <td>400.txt</td>\n",
       "      <td>US cyber security chief resigns</td>\n",
       "      <td>The man making sure US computer networks are ...</td>\n",
       "      <td>man making sure u computer network safe secure...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2224</th>\n",
       "      <td>tech</td>\n",
       "      <td>401.txt</td>\n",
       "      <td>Losing yourself in online gaming</td>\n",
       "      <td>Online role playing games are time-consuming,...</td>\n",
       "      <td>online role playing game time-consuming, enthr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2225 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      category filename                              title  \\\n",
       "0     business  001.txt  Ad sales boost Time Warner profit   \n",
       "1     business  002.txt   Dollar gains on Greenspan speech   \n",
       "2     business  003.txt  Yukos unit buyer faces loan claim   \n",
       "3     business  004.txt  High fuel prices hit BA's profits   \n",
       "4     business  005.txt  Pernod takeover talk lifts Domecq   \n",
       "...        ...      ...                                ...   \n",
       "2220      tech  397.txt   BT program to beat dialler scams   \n",
       "2221      tech  398.txt    Spam e-mails tempt net shoppers   \n",
       "2222      tech  399.txt            Be careful how you code   \n",
       "2223      tech  400.txt    US cyber security chief resigns   \n",
       "2224      tech  401.txt   Losing yourself in online gaming   \n",
       "\n",
       "                                                content  \\\n",
       "0      Quarterly profits at US media giant TimeWarne...   \n",
       "1      The dollar has hit its highest level against ...   \n",
       "2      The owners of embattled Russian oil giant Yuk...   \n",
       "3      British Airways has blamed high fuel prices f...   \n",
       "4      Shares in UK drinks and food firm Allied Dome...   \n",
       "...                                                 ...   \n",
       "2220   BT is introducing two initiatives to help bea...   \n",
       "2221   Computer users across the world continue to i...   \n",
       "2222   A new European directive could put software w...   \n",
       "2223   The man making sure US computer networks are ...   \n",
       "2224   Online role playing games are time-consuming,...   \n",
       "\n",
       "                                            new content  \n",
       "0     quarterly profit u medium giant timewarner jum...  \n",
       "1     dollar hit highest level euro almost three mon...  \n",
       "2     owner embattled russian oil giant yukos ask bu...  \n",
       "3     british airway blamed high fuel price 40% drop...  \n",
       "4     share uk drink food firm allied domecq risen s...  \n",
       "...                                                 ...  \n",
       "2220  bt introducing two initiative help beat rogue ...  \n",
       "2221  computer user across world continue ignore sec...  \n",
       "2222  new european directive could put software writ...  \n",
       "2223  man making sure u computer network safe secure...  \n",
       "2224  online role playing game time-consuming, enthr...  \n",
       "\n",
       "[2225 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = \"bbc-news-data.csv\"\n",
    "data = pd.read_csv(file,sep=\"\\t\")\n",
    "\n",
    "def tokenize_and_lemmatize(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = text.encode('ascii', 'ignore').decode('ascii')\n",
    "    words = nltk.tokenize.WhitespaceTokenizer().tokenize(text)\n",
    "    filtered_words = [word.lower() for word in words if word.lower() not in stop_words]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_words]\n",
    "    return ' '.join(lemmatized_words)\n",
    "\n",
    "data['new content'] = data['content'].apply(tokenize_and_lemmatize)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regresion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\prpaj\\Desktop\\projekti\\python\\z4\\New folder\\first_task\\praksa\\praksa_env\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "X = data['content']\n",
    "y = data['category']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "X_test_vectorized = vectorizer.transform(X_test)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_vectorized, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test_vectorized)\n",
    "\n",
    "file = 'logisticregresion.pkl'\n",
    "pickle.dump(model,open(file,'wb'))\n",
    "vectorized_file = 'vectorizer1.pkl'\n",
    "pickle.dump(vectorizer,open(vectorized_file,'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['new content']\n",
    "y = data['category']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "X_test_vectorized = vectorizer.transform(X_test)\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train_vectorized, y_train)\n",
    "\n",
    "filename = 'naivebayes.pkl'\n",
    "filename_vectorizer = 'vectorizer2.pkl'\n",
    "pickle.dump(model,open(filename,'wb'))\n",
    "pickle.dump(vectorizer,open(filename_vectorizer,'wb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[\"new content\"]\n",
    "y = data[\"category\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2)\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_features = vectorizer.fit_transform(X_train)\n",
    "X_test_features = vectorizer.transform(X_test)\n",
    "\n",
    "kneighbour = KNeighborsClassifier()\n",
    "kneighbour.fit(X_train_features, y_train)\n",
    "\n",
    "file = 'kneighbour.pkl'\n",
    "pickle.dump(kneighbour,open(file,'wb'))\n",
    "vectorized_file = 'vectorizer3.pkl'\n",
    "pickle.dump(vectorizer,open(vectorized_file,'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[\"content\"]\n",
    "y = data[\"category\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3)\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_features = vectorizer.fit_transform(X_train)\n",
    "X_test_features = vectorizer.transform(X_test)\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train_features, y_train)\n",
    "\n",
    "file = 'randomforest.pkl'\n",
    "pickle.dump(model,open(file,'wb'))\n",
    "vectorized_file = 'vectorizer4.pkl'\n",
    "pickle.dump(vectorizer,open(vectorized_file,'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[\"new content\"]\n",
    "y = data[\"category\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2)\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_features = vectorizer.fit_transform(X_train)\n",
    "X_test_features = vectorizer.transform(X_test)\n",
    "\n",
    "model = SVC()\n",
    "model.fit(X_train_features, y_train)\n",
    "\n",
    "filename='finalised_model.pkl'\n",
    "pickle.dump(model,open(filename,'wb'))\n",
    "vectorized_file='vectorizer5.pkl'\n",
    "pickle.dump(vectorizer,open(vectorized_file,'wb'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "praksa_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
