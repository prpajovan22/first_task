{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np;\n",
    "import pandas as pd;\n",
    "import csv\n",
    "import re\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import \tWordNetLemmatizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>filename</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>new content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>business</td>\n",
       "      <td>001.txt</td>\n",
       "      <td>Ad sales boost Time Warner profit</td>\n",
       "      <td>Quarterly profits at US media giant TimeWarne...</td>\n",
       "      <td>quarterly profit at us medium giant timewarner...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>002.txt</td>\n",
       "      <td>Dollar gains on Greenspan speech</td>\n",
       "      <td>The dollar has hit its highest level against ...</td>\n",
       "      <td>the dollar ha hit it highest level against the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>business</td>\n",
       "      <td>003.txt</td>\n",
       "      <td>Yukos unit buyer faces loan claim</td>\n",
       "      <td>The owners of embattled Russian oil giant Yuk...</td>\n",
       "      <td>the owner of embattled russian oil giant yukos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>business</td>\n",
       "      <td>004.txt</td>\n",
       "      <td>High fuel prices hit BA's profits</td>\n",
       "      <td>British Airways has blamed high fuel prices f...</td>\n",
       "      <td>british airways ha blamed high fuel price for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>business</td>\n",
       "      <td>005.txt</td>\n",
       "      <td>Pernod takeover talk lifts Domecq</td>\n",
       "      <td>Shares in UK drinks and food firm Allied Dome...</td>\n",
       "      <td>shares in uk drink and food firm allied domecq...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2220</th>\n",
       "      <td>tech</td>\n",
       "      <td>397.txt</td>\n",
       "      <td>BT program to beat dialler scams</td>\n",
       "      <td>BT is introducing two initiatives to help bea...</td>\n",
       "      <td>bt is introducing two initiative to help beat ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2221</th>\n",
       "      <td>tech</td>\n",
       "      <td>398.txt</td>\n",
       "      <td>Spam e-mails tempt net shoppers</td>\n",
       "      <td>Computer users across the world continue to i...</td>\n",
       "      <td>computer user across the world continue to ign...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2222</th>\n",
       "      <td>tech</td>\n",
       "      <td>399.txt</td>\n",
       "      <td>Be careful how you code</td>\n",
       "      <td>A new European directive could put software w...</td>\n",
       "      <td>a new european directive could put software wr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2223</th>\n",
       "      <td>tech</td>\n",
       "      <td>400.txt</td>\n",
       "      <td>US cyber security chief resigns</td>\n",
       "      <td>The man making sure US computer networks are ...</td>\n",
       "      <td>the man making sure us computer network are sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2224</th>\n",
       "      <td>tech</td>\n",
       "      <td>401.txt</td>\n",
       "      <td>Losing yourself in online gaming</td>\n",
       "      <td>Online role playing games are time-consuming,...</td>\n",
       "      <td>online role playing game are time-consuming, b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2225 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      category filename                              title  \\\n",
       "0     business  001.txt  Ad sales boost Time Warner profit   \n",
       "1     business  002.txt   Dollar gains on Greenspan speech   \n",
       "2     business  003.txt  Yukos unit buyer faces loan claim   \n",
       "3     business  004.txt  High fuel prices hit BA's profits   \n",
       "4     business  005.txt  Pernod takeover talk lifts Domecq   \n",
       "...        ...      ...                                ...   \n",
       "2220      tech  397.txt   BT program to beat dialler scams   \n",
       "2221      tech  398.txt    Spam e-mails tempt net shoppers   \n",
       "2222      tech  399.txt            Be careful how you code   \n",
       "2223      tech  400.txt    US cyber security chief resigns   \n",
       "2224      tech  401.txt   Losing yourself in online gaming   \n",
       "\n",
       "                                                content  \\\n",
       "0      Quarterly profits at US media giant TimeWarne...   \n",
       "1      The dollar has hit its highest level against ...   \n",
       "2      The owners of embattled Russian oil giant Yuk...   \n",
       "3      British Airways has blamed high fuel prices f...   \n",
       "4      Shares in UK drinks and food firm Allied Dome...   \n",
       "...                                                 ...   \n",
       "2220   BT is introducing two initiatives to help bea...   \n",
       "2221   Computer users across the world continue to i...   \n",
       "2222   A new European directive could put software w...   \n",
       "2223   The man making sure US computer networks are ...   \n",
       "2224   Online role playing games are time-consuming,...   \n",
       "\n",
       "                                            new content  \n",
       "0     quarterly profit at us medium giant timewarner...  \n",
       "1     the dollar ha hit it highest level against the...  \n",
       "2     the owner of embattled russian oil giant yukos...  \n",
       "3     british airways ha blamed high fuel price for ...  \n",
       "4     shares in uk drink and food firm allied domecq...  \n",
       "...                                                 ...  \n",
       "2220  bt is introducing two initiative to help beat ...  \n",
       "2221  computer user across the world continue to ign...  \n",
       "2222  a new european directive could put software wr...  \n",
       "2223  the man making sure us computer network are sa...  \n",
       "2224  online role playing game are time-consuming, b...  \n",
       "\n",
       "[2225 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = \"bbc-news-data.csv\"\n",
    "data = pd.read_csv(file,sep=\"\\t\")\n",
    "\n",
    "def tokenize_and_lemmatize(text):\n",
    "    data.astype(str).apply(lambda x: x.str.encode('ascii', 'ignore').str.decode('ascii'))\n",
    "    words = nltk.tokenize.WhitespaceTokenizer().tokenize(text)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    " \n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return ' '.join(lemmatized_words).lower()\n",
    "\n",
    "data['new content'] = data['content'].apply(tokenize_and_lemmatize)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=data['category']\n",
    "y=data['title']\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(\n",
    "    X,y, random_state=104,test_size=0.25,shuffle=True\n",
    ")\n",
    "\n",
    "print(\"X_train:\")\n",
    "print(X_train.head())\n",
    "print(X_train.shape)\n",
    "\n",
    "print('')\n",
    "print('X_test : ')\n",
    "print(X_test.head())\n",
    "print(X_test.shape)\n",
    " \n",
    "print('')\n",
    "print('y_train : ')\n",
    "print(y_train.head())\n",
    "print(y_train.shape)\n",
    " \n",
    "print('')\n",
    "print('y_test : ')\n",
    "print(y_test.head())\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = data.drop(['filename'],axis=\"columns\",inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies = pd.get_dummies(data['category'])\n",
    "dummies.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = pd.concat([inputs,dummies],axis=\"columns\")\n",
    "inputs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs.drop('sport',axis='columns',inplace=True)\n",
    "inputs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9775280898876404\n",
      "Predicted category for 'I am going to become world's best basketball player' is: sport\n"
     ]
    }
   ],
   "source": [
    "X = data['new content']\n",
    "y = data['category']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "X_test_vectorized = vectorizer.transform(X_test)\n",
    "\n",
    "nb_classifier = MultinomialNB()\n",
    "nb_classifier.fit(X_train_vectorized, y_train)\n",
    "\n",
    "y_pred = nb_classifier.predict(X_test_vectorized)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "content = \"I am going to become world's best basketball player\"\n",
    "content_vectorized = vectorizer.transform([content])\n",
    "predicted_category = nb_classifier.predict(content_vectorized)\n",
    "print(\"Predicted category for '{}' is: {}\".format(content, predicted_category[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['new content']\n",
    "y = data['category']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "X_test_vectorized = vectorizer.transform(X_test)\n",
    "\n",
    "nb_classifier = MultinomialNB()\n",
    "nb_classifier.fit(X_train_vectorized, y_train)\n",
    "\n",
    "predicted_test_category = nb_classifier.predict(X_test_vectorized)\n",
    "\n",
    "predictions_df = pd.DataFrame({'predicted_category': predicted_test_category}, index=X_test.index)\n",
    "\n",
    "data_with_predictions = pd.merge(data, predictions_df, left_index=True, right_index=True)\n",
    "\n",
    "sorted_data = data_with_predictions.sort_values(by='predicted_category')\n",
    "\n",
    "sorted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(columns=['category'], axis=1)\n",
    "y = data['category']\n",
    "\n",
    "X_encoded = pd.get_dummies(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.3)\n",
    "\n",
    "rf = MultinomialNB()\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "predictions = rf.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, predictions))\n",
    "print(\"\\n\")\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical = [var for var in data.columns if data[var].dtype=='O']\n",
    "\n",
    "data[categorical].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9820224719101124\n",
      "Predicted category for 'Jovan is a businessman' is: sport\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\prpaj\\Desktop\\projekti\\python\\z4\\New folder\\first_task\\praksa\\praksa_env\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "X = data['content']\n",
    "y = data['category']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "X_test_vectorized = vectorizer.transform(X_test)\n",
    "\n",
    "lg_classifier = LogisticRegression()\n",
    "lg_classifier.fit(X_train_vectorized, y_train)\n",
    "\n",
    "y_pred = lg_classifier.predict(X_test_vectorized)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "def predict_category(text):\n",
    "    text_vectorized = vectorizer.transform([text])\n",
    "    predicted_category = lg_classifier.predict(text_vectorized)\n",
    "    return predicted_category[0]\n",
    "\n",
    "text_to_predict = \"Jovan is a businessman\" \n",
    "predicted_category = predict_category(text_to_predict)\n",
    "print(\"Predicted category for '{}' is: {}\".format(text_to_predict, predicted_category))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(columns=['category'], axis=1)\n",
    "y = data['category']\n",
    "\n",
    "X_encoded = pd.get_dummies(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.3)\n",
    "\n",
    "lg = LogisticRegression()\n",
    "lg.fit(X_train, y_train)\n",
    "\n",
    "predictions = lg.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, predictions))\n",
    "print(\"\\n\")\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(columns=['category'], axis=1)\n",
    "y = data['category']\n",
    "\n",
    "X_encoded = pd.get_dummies(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.3)\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "predictions = rf.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, predictions))\n",
    "print(\"\\n\")\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[\"content\"]\n",
    "y = data[\"category\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3)\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_features = vectorizer.fit_transform(X_train)\n",
    "X_test_features = vectorizer.transform(X_test)\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train_features, y_train)\n",
    "\n",
    "new_text = \"This computer is great\"\n",
    "\n",
    "new_text_features = vectorizer.transform([new_text])\n",
    "\n",
    "predicted_category = model.predict(new_text_features)[0]\n",
    "\n",
    "print(f\"Predicted category for '{new_text}': {predicted_category}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X = tfidf_vectorizer.fit_transform(data['content'])\n",
    "y = data['category']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "svm_classifier = SVC(kernel='linear')\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = svm_classifier.predict(X_test)\n",
    "\n",
    "data['predicted_category'] = svm_classifier.predict(tfidf_vectorizer.transform(data['content']))\n",
    "\n",
    "sorted_data = data.sort_values(by='predicted_category')\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "X,Y = make_blobs(n_samples=500,centers =2, random_state=0,cluster_std=0.40)\n",
    "\n",
    "plt.scatter(X[:,0],X[:,1], c=Y,s=50,cmap='spring')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xfit = np.linspace(-1, 3.5)\n",
    "\n",
    "plt.scatter(X[:,0],X[:,1],c=Y,s=50, cmap='spring')\n",
    "\n",
    "for m,b,d in [(1, 0.65, 0.33), (0.5, 1.6, 0.55), (-0.2, 2.9, 0.2)]:\n",
    "    yfit = m*xfit+b\n",
    "    plt.plot(xfit, yfit,'-k')\n",
    "    plt.fill_between(xfit,yfit -d,yfit + d,edgecolor='none',\n",
    "    color='#AAAAAA',alpha=0.4)\n",
    "\n",
    "plt.xlim(-1,3.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[\"new content\"]\n",
    "y = data[\"category\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2)\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_features = vectorizer.fit_transform(X_train)\n",
    "X_test_features = vectorizer.transform(X_test)\n",
    "\n",
    "model = KNeighborsClassifier()\n",
    "model.fit(X_train_features, y_train)\n",
    "\n",
    "new_text = \"This computer is great\"\n",
    "new_text_features = vectorizer.transform([new_text])\n",
    "predicted_category = model.predict(new_text_features)[0]\n",
    "\n",
    "print(f\"Predicted category for '{new_text}': {predicted_category}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(columns=['category'], axis=1)\n",
    "y = data['category']\n",
    "\n",
    "X_encoded = pd.get_dummies(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.3)\n",
    "\n",
    "rf = KNeighborsClassifier()\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "predictions = rf.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, predictions))\n",
    "print(\"\\n\")\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[\"new content\"]\n",
    "y = data[\"category\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2)\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_features = vectorizer.fit_transform(X_train)\n",
    "X_test_features = vectorizer.transform(X_test)\n",
    "\n",
    "model = SVC()\n",
    "model.fit(X_train_features, y_train)\n",
    "\n",
    "new_text = \"The new phones are all the rage, they are getting better then the computers\"\n",
    "new_text_features = vectorizer.transform([new_text])\n",
    "predicted_category = model.predict(new_text_features)[0]\n",
    "\n",
    "print(f\"Predicted category for '{new_text}': {predicted_category}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(columns=['category'], axis=1)\n",
    "y = data['category']\n",
    "\n",
    "X_encoded = pd.get_dummies(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.3)\n",
    "\n",
    "rf = SVC()\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "predictions = rf.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, predictions))\n",
    "print(\"\\n\")\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Picke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[\"new content\"]\n",
    "y = data[\"category\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2)\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_features = vectorizer.fit_transform(X_train)\n",
    "X_test_features = vectorizer.transform(X_test)\n",
    "\n",
    "kneighbour = KNeighborsClassifier()\n",
    "kneighbour.fit(X_train_features, y_train)\n",
    "\n",
    "file = 'kneighbour.pkl'\n",
    "pickle.dump(kneighbour,open(file,'wb'))\n",
    "vectorized_file = 'knvectoriser.pkl'\n",
    "pickle.dump(vectorizer,open(vectorized_file,'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted category for 'This computer is great': tech\n"
     ]
    }
   ],
   "source": [
    "filename = 'kneighbour.pkl'\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "\n",
    "new_text = \"This computer is great\"\n",
    "new_text_features = vectorizer.transform([new_text])\n",
    "predicted_category = loaded_model.predict(new_text_features)[0]\n",
    "\n",
    "print(f\"Predicted category for '{new_text}': {predicted_category}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[\"new content\"]\n",
    "y = data[\"category\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2)\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_features = vectorizer.fit_transform(X_train)\n",
    "X_test_features = vectorizer.transform(X_test)\n",
    "\n",
    "model = SVC()\n",
    "model.fit(X_train_features, y_train)\n",
    "\n",
    "filename='finalised_model.pkl'\n",
    "pickle.dump(model,open(filename,'wb'))\n",
    "vectorized_file='svcvectorised.pkl'\n",
    "pickle.dump(vectorizer,open(vectorized_file,'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted category for A world famous singer is going to do another concert: entertainment\n"
     ]
    }
   ],
   "source": [
    "filename = 'finalised_model.pkl'\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "\n",
    "new_text = \"A world famous singer is going to do another concert\"\n",
    "\n",
    "new_text_features = vectorizer.transform([new_text])\n",
    "predicted_category = loaded_model.predict(new_text_features)[0]\n",
    "\n",
    "print(f\"Predicted category for {new_text}:\", predicted_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['new content']\n",
    "y = data['category']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "X_test_vectorized = vectorizer.transform(X_test)\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train_vectorized, y_train)\n",
    "\n",
    "filename = 'naivebayes.pkl'\n",
    "filename_vectorizer = 'vectorizer.pkl'\n",
    "pickle.dump(model,open(filename,'wb'))\n",
    "pickle.dump(vectorizer,open(filename_vectorizer,'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted category for 'The new phones are all the rage, they are getting better then the computers' is: tech\n"
     ]
    }
   ],
   "source": [
    "filename = 'naivebayes.pkl'\n",
    "loaded_model = pickle.load(open(filename,'rb'))\n",
    "\n",
    "y_pred = model.predict(X_test_vectorized)\n",
    "\n",
    "content = \"The new phones are all the rage, they are getting better then the computers\"\n",
    "content_vectorized = vectorizer.transform([content])\n",
    "predicted_category = model.predict(content_vectorized)\n",
    "print(\"Predicted category for '{}' is: {}\".format(content, predicted_category[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[\"content\"]\n",
    "y = data[\"category\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3)\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_features = vectorizer.fit_transform(X_train)\n",
    "X_test_features = vectorizer.transform(X_test)\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train_features, y_train)\n",
    "\n",
    "file = 'randomforest.pkl'\n",
    "pickle.dump(model,open(file,'wb'))\n",
    "vectorized_file = 'rfvectorizer.pkl'\n",
    "pickle.dump(vectorizer,open(vectorized_file,'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted category for 'The dollar is doing perfectly': sport\n"
     ]
    }
   ],
   "source": [
    "filename = 'randomforest.pkl'\n",
    "loaded_model = pickle.load(open(filename,'rb'))\n",
    "\n",
    "new_text = \"The dollar is doing perfectly\"\n",
    "new_text_features = vectorizer.transform([new_text])\n",
    "predicted_category = loaded_model.predict(new_text_features)[0]\n",
    "\n",
    "print(f\"Predicted category for '{new_text}': {predicted_category}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\prpaj\\Desktop\\projekti\\python\\z4\\New folder\\first_task\\praksa\\praksa_env\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "X = data['content']\n",
    "y = data['category']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "X_test_vectorized = vectorizer.transform(X_test)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_vectorized, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test_vectorized)\n",
    "\n",
    "file = 'logisticregresion.pkl'\n",
    "pickle.dump(model,open(file,'wb'))\n",
    "vectorized_file = 'vectorised.pkl'\n",
    "pickle.dump(vectorizer,open(vectorized_file,'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted category for 'Jovan is a businessman' is: sport\n"
     ]
    }
   ],
   "source": [
    "filename = 'logisticregresion.pkl'\n",
    "loaded_model = pickle.load(open(filename,'rb'))\n",
    "\n",
    "def predict_category(text):\n",
    "    text_vectorized = vectorizer.transform([text])\n",
    "    predicted_category = loaded_model.predict(text_vectorized)\n",
    "    return predicted_category[0]\n",
    "\n",
    "text_to_predict = \"Jovan is a businessman\" \n",
    "predicted_category = predict_category(text_to_predict)\n",
    "print(\"Predicted category for '{}' is: {}\".format(text_to_predict, predicted_category))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chosing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = input(\"Input the text you want to categorise: \")\n",
    "def question():\n",
    "    print(\"which machine learning model do you want to use: \")\n",
    "    print(\"1)Logistic Regression\")\n",
    "    print(\"2)Naive Bayes \")\n",
    "    print(\"3)K-Nearest Neighbors\")\n",
    "    print(\"4)Random Forest\")\n",
    "    print(\"5)Support Vector Machine\")\n",
    "\n",
    "question()\n",
    "\n",
    "def sol(**kwargs):\n",
    "    ml = input(\"Choose which model you want to use: \")\n",
    "    if ml not in {'1', '2', '3', '4', '5'}:\n",
    "        print(\"Invalid choice!\")\n",
    "        question()\n",
    "        return sol(**kwargs)\n",
    "    else:\n",
    "        if ml == '1':\n",
    "            filename = 'logisticregresion.pkl'\n",
    "            filename_vectorizer = 'vectorised.pkl'\n",
    "        elif ml == '2':\n",
    "            filename = 'naivebayes.sav'\n",
    "            filename_vectorizer = 'vectorizer.pkl'\n",
    "        elif ml == '3':\n",
    "            filename = 'kneighbour.pkl'\n",
    "            filename_vectorizer = 'knvectoriser.pkl'\n",
    "        elif ml == '4':\n",
    "            filename = 'randomforest.pkl'\n",
    "            filename_vectorizer = 'rfvectorizer.pkl'\n",
    "        elif ml == '5':\n",
    "            filename = 'finalised_model.pkl'\n",
    "            filename_vectorizer = 'svcvectorised.pkl'\n",
    "\n",
    "    loaded_model = pickle.load(open(filename, 'rb'))\n",
    "    loaded_vectorizer = pickle.load(open(filename_vectorizer, 'rb'))\n",
    "\n",
    "    text_vectorized = loaded_vectorizer.transform([kwargs['context']])\n",
    "    predicted_category = loaded_model.predict(text_vectorized)[0]\n",
    "    print(f\"Predicted category for '{kwargs['context']}': {predicted_category}\")\n",
    "\n",
    "context = input(\"Input the text you want to categorise: \")\n",
    "question()\n",
    "sol(context=context)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "praksa_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
