{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m svm\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msvm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SVC\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstem\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PorterStemmer\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstem\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \tWordNetLemmatizer\n",
      "File \u001b[1;32mc:\\Users\\prpaj\\Desktop\\projekti\\python\\z4\\New folder\\first_task\\praksa\\praksa_env\\Lib\\site-packages\\nltk\\__init__.py:146\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mjsontags\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;66;03m###########################################################\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;66;03m# PACKAGES\u001b[39;00m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;66;03m###########################################################\u001b[39;00m\n\u001b[1;32m--> 146\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchunk\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclassify\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minference\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\prpaj\\Desktop\\projekti\\python\\z4\\New folder\\first_task\\praksa\\praksa_env\\Lib\\site-packages\\nltk\\chunk\\__init__.py:155\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Natural Language Toolkit: Chunkers\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Copyright (C) 2001-2023 NLTK Project\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# For license information, see LICENSE.TXT\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;124;03mClasses and interfaces for identifying non-overlapping linguistic\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;124;03mgroups (such as base noun phrases) in unrestricted text.  This task is\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;124;03m     pattern is valid.\u001b[39;00m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 155\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchunk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChunkParserI\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchunk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregexp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RegexpChunkParser, RegexpParser\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchunk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m    158\u001b[0m     ChunkScore,\n\u001b[0;32m    159\u001b[0m     accuracy,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    165\u001b[0m     tree2conlltags,\n\u001b[0;32m    166\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\prpaj\\Desktop\\projekti\\python\\z4\\New folder\\first_task\\praksa\\praksa_env\\Lib\\site-packages\\nltk\\chunk\\api.py:15\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchunk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChunkScore\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minternals\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deprecated\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ParserI\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mChunkParserI\u001b[39;00m(ParserI):\n\u001b[0;32m     19\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;124;03m    A processing interface for identifying non-overlapping groups in\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;124;03m    unrestricted text.  Typically, chunk parsers are used to find base\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;124;03m    will always generate a parse.\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\prpaj\\Desktop\\projekti\\python\\z4\\New folder\\first_task\\praksa\\praksa_env\\Lib\\site-packages\\nltk\\parse\\__init__.py:100\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparse\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrecursivedescent\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     96\u001b[0m     RecursiveDescentParser,\n\u001b[0;32m     97\u001b[0m     SteppingRecursiveDescentParser,\n\u001b[0;32m     98\u001b[0m )\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparse\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mshiftreduce\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ShiftReduceParser, SteppingShiftReduceParser\n\u001b[1;32m--> 100\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparse\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransitionparser\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TransitionParser\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparse\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TestGrammar, extract_test_sentences, load_parser\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparse\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mviterbi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ViterbiParser\n",
      "File \u001b[1;32mc:\\Users\\prpaj\\Desktop\\projekti\\python\\z4\\New folder\\first_task\\praksa\\praksa_env\\Lib\\site-packages\\nltk\\parse\\transitionparser.py:19\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sparse\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m svm\n\u001b[1;32m---> 19\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_svmlight_file\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\prpaj\\Desktop\\projekti\\python\\z4\\New folder\\first_task\\praksa\\praksa_env\\Lib\\site-packages\\sklearn\\datasets\\__init__.py:25\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_kddcup99\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fetch_kddcup99\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_lfw\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fetch_lfw_pairs, fetch_lfw_people\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_olivetti_faces\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fetch_olivetti_faces\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_openml\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fetch_openml\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_rcv1\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fetch_rcv1\n",
      "File \u001b[1;32mc:\\Users\\prpaj\\Desktop\\projekti\\python\\z4\\New folder\\first_task\\praksa\\praksa_env\\Lib\\site-packages\\sklearn\\datasets\\_olivetti_faces.py:21\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjoblib\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m loadmat\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Bunch, check_random_state\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m validate_params\n",
      "File \u001b[1;32mc:\\Users\\prpaj\\Desktop\\projekti\\python\\z4\\New folder\\first_task\\praksa\\praksa_env\\Lib\\site-packages\\scipy\\io\\__init__.py:97\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03m==================================\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03mInput and output (:mod:`scipy.io`)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;124;03m   ParseArffError\u001b[39;00m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;66;03m# matfile read and write\u001b[39;00m\n\u001b[1;32m---> 97\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmatlab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m loadmat, savemat, whosmat\n\u001b[0;32m     99\u001b[0m \u001b[38;5;66;03m# netCDF file support\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_netcdf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m netcdf_file, netcdf_variable\n",
      "File \u001b[1;32mc:\\Users\\prpaj\\Desktop\\projekti\\python\\z4\\New folder\\first_task\\praksa\\praksa_env\\Lib\\site-packages\\scipy\\io\\matlab\\__init__.py:45\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03mMATLAB® file utilities (:mod:`scipy.io.matlab`)\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m===============================================\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     42\u001b[0m \n\u001b[0;32m     43\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Matlab file read and write utilities\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_mio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m loadmat, savemat, whosmat\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_mio5\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MatlabFunction\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_mio5_params\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MatlabObject, MatlabOpaque, mat_struct\n",
      "File \u001b[1;32mc:\\Users\\prpaj\\Desktop\\projekti\\python\\z4\\New folder\\first_task\\praksa\\praksa_env\\Lib\\site-packages\\scipy\\io\\matlab\\_mio.py:8\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Authors: Travis Oliphant, Matthew Brett\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcontextlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m contextmanager\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_miobase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _get_matfile_version, docfiller\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_mio4\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MatFile4Reader, MatFile4Writer\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_mio5\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MatFile5Reader, MatFile5Writer\n",
      "File \u001b[1;32mc:\\Users\\prpaj\\Desktop\\projekti\\python\\z4\\New folder\\first_task\\praksa\\praksa_env\\Lib\\site-packages\\scipy\\io\\matlab\\_miobase.py:12\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_lib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m doccer\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _byteordercodes \u001b[38;5;28;01mas\u001b[39;00m boc\n\u001b[0;32m     14\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMatFileReader\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMatReadError\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMatReadWarning\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMatVarReader\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMatWriteError\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124marr_dtype_number\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmatdims\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mread_dtype\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     20\u001b[0m ]\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mMatReadError\u001b[39;00m(\u001b[38;5;167;01mException\u001b[39;00m):\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1360\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1322\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1262\u001b[0m, in \u001b[0;36m_find_spec\u001b[1;34m(name, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1524\u001b[0m, in \u001b[0;36mfind_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1498\u001b[0m, in \u001b[0;36m_get_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1597\u001b[0m, in \u001b[0;36mfind_spec\u001b[1;34m(self, fullname, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:147\u001b[0m, in \u001b[0;36m_path_stat\u001b[1;34m(path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np;\n",
    "import pandas as pd;\n",
    "import csv\n",
    "import re\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import \tWordNetLemmatizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"bbc-news-data.csv\"\n",
    "data = pd.read_csv(file,sep=\"\\t\")\n",
    "\n",
    "def tokenize_and_lemmatize(text):\n",
    "    data.astype(str).apply(lambda x: x.str.encode('ascii', 'ignore').str.decode('ascii'))\n",
    "    words = nltk.tokenize.WhitespaceTokenizer().tokenize(text)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    " \n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return ' '.join(lemmatized_words).lower()\n",
    "\n",
    "data['new content'] = data['content'].apply(tokenize_and_lemmatize)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=data['category']\n",
    "y=data['title']\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(\n",
    "    X,y, random_state=104,test_size=0.25,shuffle=True\n",
    ")\n",
    "\n",
    "print(\"X_train:\")\n",
    "print(X_train.head())\n",
    "print(X_train.shape)\n",
    "\n",
    "print('')\n",
    "print('X_test : ')\n",
    "print(X_test.head())\n",
    "print(X_test.shape)\n",
    " \n",
    "print('')\n",
    "print('y_train : ')\n",
    "print(y_train.head())\n",
    "print(y_train.shape)\n",
    " \n",
    "print('')\n",
    "print('y_test : ')\n",
    "print(y_test.head())\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = data.drop(['filename'],axis=\"columns\",inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies = pd.get_dummies(data['category'])\n",
    "dummies.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = pd.concat([inputs,dummies],axis=\"columns\")\n",
    "inputs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs.drop('sport',axis='columns',inplace=True)\n",
    "inputs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['new content']\n",
    "y = data['category']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "X_test_vectorized = vectorizer.transform(X_test)\n",
    "\n",
    "nb_classifier = MultinomialNB()\n",
    "nb_classifier.fit(X_train_vectorized, y_train)\n",
    "\n",
    "y_pred = nb_classifier.predict(X_test_vectorized)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "content = \"I am going to become world's best basketball player\"\n",
    "content_vectorized = vectorizer.transform([content])\n",
    "predicted_category = nb_classifier.predict(content_vectorized)\n",
    "print(\"Predicted category for '{}' is: {}\".format(content, predicted_category[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['new content']\n",
    "y = data['category']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "X_test_vectorized = vectorizer.transform(X_test)\n",
    "\n",
    "nb_classifier = MultinomialNB()\n",
    "nb_classifier.fit(X_train_vectorized, y_train)\n",
    "\n",
    "predicted_test_category = nb_classifier.predict(X_test_vectorized)\n",
    "\n",
    "predictions_df = pd.DataFrame({'predicted_category': predicted_test_category}, index=X_test.index)\n",
    "\n",
    "data_with_predictions = pd.merge(data, predictions_df, left_index=True, right_index=True)\n",
    "\n",
    "sorted_data = data_with_predictions.sort_values(by='predicted_category')\n",
    "\n",
    "sorted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(columns=['category'], axis=1)\n",
    "y = data['category']\n",
    "\n",
    "X_encoded = pd.get_dummies(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.3)\n",
    "\n",
    "rf = MultinomialNB()\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "predictions = rf.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, predictions))\n",
    "print(\"\\n\")\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical = [var for var in data.columns if data[var].dtype=='O']\n",
    "\n",
    "data[categorical].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['content']\n",
    "y = data['category']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "X_test_vectorized = vectorizer.transform(X_test)\n",
    "\n",
    "lg_classifier = LogisticRegression()\n",
    "lg_classifier.fit(X_train_vectorized, y_train)\n",
    "\n",
    "y_pred = lg_classifier.predict(X_test_vectorized)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "def predict_category(text):\n",
    "    text_vectorized = vectorizer.transform([text])\n",
    "    predicted_category = lg_classifier.predict(text_vectorized)\n",
    "    return predicted_category[0]\n",
    "\n",
    "text_to_predict = \"Jovan is a businessman\" \n",
    "predicted_category = predict_category(text_to_predict)\n",
    "print(\"Predicted category for '{}' is: {}\".format(text_to_predict, predicted_category))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(columns=['category'], axis=1)\n",
    "y = data['category']\n",
    "\n",
    "X_encoded = pd.get_dummies(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.3)\n",
    "\n",
    "lg = LogisticRegression()\n",
    "lg.fit(X_train, y_train)\n",
    "\n",
    "predictions = lg.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, predictions))\n",
    "print(\"\\n\")\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(columns=['category'], axis=1)\n",
    "y = data['category']\n",
    "\n",
    "X_encoded = pd.get_dummies(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.3)\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "predictions = rf.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, predictions))\n",
    "print(\"\\n\")\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[\"content\"]\n",
    "y = data[\"category\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3)\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_features = vectorizer.fit_transform(X_train)\n",
    "X_test_features = vectorizer.transform(X_test)\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train_features, y_train)\n",
    "\n",
    "new_text = \"This computer is great\"\n",
    "\n",
    "new_text_features = vectorizer.transform([new_text])\n",
    "\n",
    "predicted_category = model.predict(new_text_features)[0]\n",
    "\n",
    "print(f\"Predicted category for '{new_text}': {predicted_category}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X = tfidf_vectorizer.fit_transform(data['content'])\n",
    "y = data['category']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "svm_classifier = SVC(kernel='linear')\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = svm_classifier.predict(X_test)\n",
    "\n",
    "data['predicted_category'] = svm_classifier.predict(tfidf_vectorizer.transform(data['content']))\n",
    "\n",
    "sorted_data = data.sort_values(by='predicted_category')\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "X,Y = make_blobs(n_samples=500,centers =2, random_state=0,cluster_std=0.40)\n",
    "\n",
    "plt.scatter(X[:,0],X[:,1], c=Y,s=50,cmap='spring')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xfit = np.linspace(-1, 3.5)\n",
    "\n",
    "plt.scatter(X[:,0],X[:,1],c=Y,s=50, cmap='spring')\n",
    "\n",
    "for m,b,d in [(1, 0.65, 0.33), (0.5, 1.6, 0.55), (-0.2, 2.9, 0.2)]:\n",
    "    yfit = m*xfit+b\n",
    "    plt.plot(xfit, yfit,'-k')\n",
    "    plt.fill_between(xfit,yfit -d,yfit + d,edgecolor='none',\n",
    "    color='#AAAAAA',alpha=0.4)\n",
    "\n",
    "plt.xlim(-1,3.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[\"new content\"]\n",
    "y = data[\"category\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2)\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_features = vectorizer.fit_transform(X_train)\n",
    "X_test_features = vectorizer.transform(X_test)\n",
    "\n",
    "model = KNeighborsClassifier()\n",
    "model.fit(X_train_features, y_train)\n",
    "\n",
    "new_text = \"This computer is great\"\n",
    "new_text_features = vectorizer.transform([new_text])\n",
    "predicted_category = model.predict(new_text_features)[0]\n",
    "\n",
    "print(f\"Predicted category for '{new_text}': {predicted_category}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(columns=['category'], axis=1)\n",
    "y = data['category']\n",
    "\n",
    "X_encoded = pd.get_dummies(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.3)\n",
    "\n",
    "rf = KNeighborsClassifier()\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "predictions = rf.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, predictions))\n",
    "print(\"\\n\")\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[\"new content\"]\n",
    "y = data[\"category\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2)\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_features = vectorizer.fit_transform(X_train)\n",
    "X_test_features = vectorizer.transform(X_test)\n",
    "\n",
    "model = SVC()\n",
    "model.fit(X_train_features, y_train)\n",
    "\n",
    "new_text = \"The new phones are all the rage, they are getting better then the computers\"\n",
    "new_text_features = vectorizer.transform([new_text])\n",
    "predicted_category = model.predict(new_text_features)[0]\n",
    "\n",
    "print(f\"Predicted category for '{new_text}': {predicted_category}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(columns=['category'], axis=1)\n",
    "y = data['category']\n",
    "\n",
    "X_encoded = pd.get_dummies(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.3)\n",
    "\n",
    "rf = SVC()\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "predictions = rf.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, predictions))\n",
    "print(\"\\n\")\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Picke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[\"new content\"]\n",
    "y = data[\"category\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2)\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_features = vectorizer.fit_transform(X_train)\n",
    "X_test_features = vectorizer.transform(X_test)\n",
    "\n",
    "kneighbour = KNeighborsClassifier()\n",
    "kneighbour.fit(X_train_features, y_train)\n",
    "\n",
    "file = 'kneighbour.pkl'\n",
    "pickle.dump(kneighbour,open(file,'wb'))\n",
    "vectorized_file = 'knvectoriser.pkl'\n",
    "pickle.dump(vectorizer,open(vectorized_file,'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted category for 'This computer is great': tech\n"
     ]
    }
   ],
   "source": [
    "filename = 'kneighbour.pkl'\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "\n",
    "new_text = \"This computer is great\"\n",
    "new_text_features = vectorizer.transform([new_text])\n",
    "predicted_category = loaded_model.predict(new_text_features)[0]\n",
    "\n",
    "print(f\"Predicted category for '{new_text}': {predicted_category}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[\"new content\"]\n",
    "y = data[\"category\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2)\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_features = vectorizer.fit_transform(X_train)\n",
    "X_test_features = vectorizer.transform(X_test)\n",
    "\n",
    "model = SVC()\n",
    "model.fit(X_train_features, y_train)\n",
    "\n",
    "filename='finalised_model.pkl'\n",
    "pickle.dump(model,open(filename,'wb'))\n",
    "vectorized_file='svcvectorised.pkl'\n",
    "pickle.dump(vectorizer,open(vectorized_file,'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted category for A world famous singer is going to do another concert: entertainment\n"
     ]
    }
   ],
   "source": [
    "filename = 'finalised_model.pkl'\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "\n",
    "new_text = \"A world famous singer is going to do another concert\"\n",
    "\n",
    "new_text_features = vectorizer.transform([new_text])\n",
    "predicted_category = loaded_model.predict(new_text_features)[0]\n",
    "\n",
    "print(f\"Predicted category for {new_text}:\", predicted_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['new content']\n",
    "y = data['category']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "X_test_vectorized = vectorizer.transform(X_test)\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train_vectorized, y_train)\n",
    "\n",
    "filename = 'naivebayes.sav'\n",
    "filename_vectorizer = 'vectorizer.pkl'\n",
    "pickle.dump(model,open(filename,'wb'))\n",
    "pickle.dump(vectorizer,open(filename_vectorizer,'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted category for 'The new phones are all the rage, they are getting better then the computers' is: tech\n"
     ]
    }
   ],
   "source": [
    "filename = 'naivebayes.sav'\n",
    "loaded_model = pickle.load(open(filename,'rb'))\n",
    "\n",
    "y_pred = model.predict(X_test_vectorized)\n",
    "\n",
    "content = \"The new phones are all the rage, they are getting better then the computers\"\n",
    "content_vectorized = vectorizer.transform([content])\n",
    "predicted_category = model.predict(content_vectorized)\n",
    "print(\"Predicted category for '{}' is: {}\".format(content, predicted_category[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[\"content\"]\n",
    "y = data[\"category\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3)\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_features = vectorizer.fit_transform(X_train)\n",
    "X_test_features = vectorizer.transform(X_test)\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train_features, y_train)\n",
    "\n",
    "file = 'randomforest.pkl'\n",
    "pickle.dump(model,open(file,'wb'))\n",
    "vectorized_file = 'rfvectorizer.pkl'\n",
    "pickle.dump(vectorizer,open(vectorized_file,'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted category for 'The dollar is doing perfectly': sport\n"
     ]
    }
   ],
   "source": [
    "filename = 'randomforest.pkl'\n",
    "loaded_model = pickle.load(open(filename,'rb'))\n",
    "\n",
    "new_text = \"The dollar is doing perfectly\"\n",
    "new_text_features = vectorizer.transform([new_text])\n",
    "predicted_category = loaded_model.predict(new_text_features)[0]\n",
    "\n",
    "print(f\"Predicted category for '{new_text}': {predicted_category}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\prpaj\\Desktop\\projekti\\python\\z4\\New folder\\first_task\\praksa\\praksa_env\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "X = data['content']\n",
    "y = data['category']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "X_test_vectorized = vectorizer.transform(X_test)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_vectorized, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test_vectorized)\n",
    "\n",
    "file = 'logisticregresion.pkl'\n",
    "pickle.dump(model,open(file,'wb'))\n",
    "vectorized_file = 'vectorised.pkl'\n",
    "pickle.dump(vectorizer,open(vectorized_file,'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted category for 'Jovan is a businessman' is: sport\n"
     ]
    }
   ],
   "source": [
    "filename = 'logisticregresion.pkl'\n",
    "loaded_model = pickle.load(open(filename,'rb'))\n",
    "\n",
    "def predict_category(text):\n",
    "    text_vectorized = vectorizer.transform([text])\n",
    "    predicted_category = loaded_model.predict(text_vectorized)\n",
    "    return predicted_category[0]\n",
    "\n",
    "text_to_predict = \"Jovan is a businessman\" \n",
    "predicted_category = predict_category(text_to_predict)\n",
    "print(\"Predicted category for '{}' is: {}\".format(text_to_predict, predicted_category))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chosing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = input(\"Input the text you want to categorise: \")\n",
    "def question():\n",
    "    print(\"which machine learning model do you want to use: \")\n",
    "    print(\"1)Logistic Regression\")\n",
    "    print(\"2)Naive Bayes \")\n",
    "    print(\"3)K-Nearest Neighbors\")\n",
    "    print(\"4)Random Forest\")\n",
    "    print(\"5)Support Vector Machine\")\n",
    "\n",
    "question()\n",
    "\n",
    "ml = input(\"Chose which model you want to use: \" )\n",
    "if ml not in {'1', '2', '3', '4', '5'}:\n",
    "    print(\"Invalid choice!\")\n",
    "    question() \n",
    "else:\n",
    "    if ml == '1':\n",
    "        filename = 'logisticregresion.pkl'\n",
    "        filename_vectorizer = 'vectorised.pkl'\n",
    "    elif ml == '2':\n",
    "        filename = 'naivebayes.sav'\n",
    "        filename_vectorizer = 'vectorizer.pkl'\n",
    "    elif ml == '3':\n",
    "        filename = 'kneighbour.pkl'\n",
    "        filename_vectorizer = 'knvectoriser.pkl'\n",
    "    elif ml == '4':\n",
    "        filename = 'randomforest.pkl'\n",
    "        filename_vectorizer = 'rfvectorizer.pkl'\n",
    "    elif ml == '5':\n",
    "        filename = 'finalised_model.pkl'\n",
    "        filename_vectorizer = 'svcvectorised.pkl'\n",
    "\n",
    "        \n",
    "loaded_model = pickle.load(open(filename,'rb'))\n",
    "loaded_vectorizer = pickle.load(open(filename_vectorizer, 'rb'))\n",
    "\n",
    "text_vectorized = loaded_vectorizer.transform([context])\n",
    "predicted_category = loaded_model.predict(text_vectorized)[0]\n",
    "print(f\"Predicted category for '{context}': {predicted_category}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "praksa_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
